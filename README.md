# SVFX: Using Random Forests to Prioritize Structural Variants

SVFX is a machine learning based tool to assign pathogenic scores to large deletions and duplications.

## Setup

Install `conda` using these [directions](https://docs.conda.io/projects/conda/en/latest/user-guide/install/macos.html), then activate the provided environment with
`conda env create -f SVFXenv.yml && conda activate SVFXenv`.

## Configuring the pipeline

The pipeline is built using [Snakemake](https://snakemake.readthedocs.io/); to configure
elements of the pipeline, modify the `config.yaml` file from the root directory.

### Configurable elements

**`run_id`**: A string to identify this run; this string will be prepended to the files generated by this run
so that multiple runs can be executed without cleanup.

**`coordinates`**: `control` and `disease` should have the file paths for the `control` and `disease` coordinates, respectively.

**`bigwig_files`**: A hyphenated list of paths to BigWig files to be used in
feature computation.

**`overlap_coordinate_files`**: A hyphenated list of paths to coordinate files for computing overlap as a feature (e.g. coordinates of certain genes)

**`normalized`**: Boolean flag for whether to length-normalize the feature values.

**`variant_type`**: What kind of variants are in the input files (either `DEL` or `DUP`)

**`ref_genome`**: Index file (`*.fai`) for a reference genome

**`randomized_num`**: If `normalized` is `True`, the number of pseudo-matrices of variants to normalize with.

**`output_matrix`**: `control` and `disease` should have the root output names for the control feature matrix and the disease feature matrix, respectively (these will be combined into a file with the root name `combined_matrix`).

**`columns_to_remove`**: File with indices for the columns in the feature matrix that should not be considered in the model (e.g. the indices of the
feature coordinates). The default file should be fine for the matrices generated from the scripts we provide.

**`class_label_index`**: Column with the class label. Again, the default value should be fine for matrices generated from our scripts.

**`num_trees`**: Number of trees in the random forest

**`max_depth`**: The maximum depth of a tree in the random forest

**`min_samples_split`**: The minimum number of samples to split an internal node in a tree in the random forest


## Running the entire pipeline

Simply run `snakemake --cores=8` from the root directory.


### Running parts of the pipeline

To just run a part of the pipeline, modify `config.yaml` and `Snakefile` with the configuration and input/output you want, then run `snakemake --cores=8 -R --until (rule name)`.

For example, if you have a list of coordinates and just want to generate the feature matrix for it (which is useful if you want to use a preexisting model to score additional variants), you can modify `Snakefile` to set the control or disease input to your file (for the other input, you can just put an arbitrary coordinates file, then delete the output matrix corresponding to that input). You can then run `snakemake --cores=8 -R --until generate_feature_matrix`, and the matrix will be generated.

## Other files

### `src/load_model.py`

Tests the model on a matrix. Note that the matrix must have the same format as the matrices used to train the model (every single feature in the same order as the training data, with no additional features). To generate a matrix given coordinates, see (here)[###running-parts-of-the-pipeline].

You will need to `pip3 install joblib` before this can be run. If any dependencies are still missing, install them with pip3.

Usage: `python3 src/load_model.py -i [input matrix] -d [File of columns to ignore for model training; the same as columns_to_remove in the config file. remove_inds.txt should be find for matrices generated from our scripts.] -m [the .pkl file with all 10 models. This is output by the Snakemake pipeline] -t [Index of the class label in the matrix - use 0 if the matrix was generated using our code] -o [filename to write the predictions to]`

### `roc_prc_gen.py`

Usage: `python3 roc_prc_gen.py [input file] [output file]`

Given a tab-separated two-column input file of predicted scores and true labels (with a header, as with the following example):

`Predicted	True`
`0.755	1`
`...`
`0.211	0`

Outputs AU(ROC/PRC) information, along with a plot of the ROC, saved to the specified output file.
